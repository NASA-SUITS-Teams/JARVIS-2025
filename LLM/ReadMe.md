# LLM

As part of our team, we created an AI assistant named JARVIS, powered by multiple open source models and integrated through this program. We aim to use audio from a microphone or text to ask JARVIS questions ranging from how to use our UI to how to aid astronauts in certain scenarios. JARVIS utilizes Retrieval-Augmented Generation to select relevant information from documents we provide and then responds both in text and audibly.


## Requirements

It is recommended to use a virtual environment.

Run `pip install` on any imports not working. For TTS, install `coqui-tts`.

With ollama, run `ollama pull gemma3:4b-it-q8_0`. The rest of the models should download on the first time running a program.


## How to run

Make sure `ollama serve` is running in a separate terminal.

To run a test, navigate to `LLM/`, then run
```
python -m tests.{test_name}
```


## File structure

```
├── tests/           # Runnable programs for individual and combined components
├── utils/           # Helper functions/classes
├── [documents/]     # Created by the user. Contains only txt files. Used for RAG
├── [vectorstore/]   # Generated by program. Delete to regenerate if updating `documents/`
└── ReadMe.md
```


## Open source models used

Faster Whisper (https://github.com/SYSTRAN/faster-whisper): Used to convert audio to text

Coqui TTS (https://github.com/idiap/coqui-ai-TTS): Used to convert text to audio

Gemma3 (https://ollama.com/library/gemma3): Large Language Model

Mxbai-embed-large (https://ollama.com/library/mxbai-embed-large): Embedding model for generating vector representations

